A mini transformer with two repeated blocks of masked multi-head attention and feed forward layers. 

Implemented based on Andrej Karpathy's "Let's Build GPT" video (https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5868s), which 
follows the "Attention is All You Need" paper (https://arxiv.org/pdf/1706.03762).

Modifications were made for clarity and reproducibility without dedicated GPU processing


Adding something here
