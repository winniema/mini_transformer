{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVVqTcYEOxYXQuPixJpwBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winniema/mini_transformer/blob/main/Multi_Headed_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K1gfV0QW0Qwm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" single head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(C, head_size, bias=False)\n",
        "        self.query = nn.Linear(C, head_size, bias=False)\n",
        "        self.value = nn.Linear(C, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# head size is the dimension which this attention head operates in, usually of a lower dimension than C\n",
        "head_size = 8\n",
        "num_heads = 4\n",
        "\n",
        "# multiple heads of self-attention\n",
        "heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "# linear projection\n",
        "proj = nn.Linear(C, C)\n",
        "\n",
        "out = [h(x) for h in heads] # [(B,T,hs), (B,T,hs), (B,T,hs), (B,T,hs)]\n",
        "out = torch.cat(out, dim=-1) # concat along the last dimension (B,T,hs*num_heads)\n",
        "out = proj(out) # (B,T,C) where C = hs*num_heads"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "* There's three motivations for a multi-headed attention architecture:\n",
        "  1. Lower computational complexity per attention head as C (n_embed) is mapped to a lower dimension num_heads\n",
        "  2. Computation can be parallelized across the heads of attention\n",
        "  3. Learning long-range dependencies (dependencies from a token from some time back) is easier when the computational path length from model input to output is shorter\n",
        "* Each attention head, represented by a query, a key, and, a value would communicate different ideas. These ideas are then concatenated together (imagine stacking them side by side along the channel dimension). This stacking builds the last dimension back to the input channel dimension.\n",
        "* The results are linearly projected to get the final output of the multi-headed attention block"
      ],
      "metadata": {
        "id": "hEEhvbh82fl4"
      }
    }
  ]
}