{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqYPcEe+Pd9lEhI1jEhRS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winniema/mini_transformer/blob/main/Single_Head_of_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "iCdIAR_cq7hB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "\n",
        "# Implement a single head of attention\n",
        "# Head size is the dimention which this attention head operates in. Usually it's of a lower dimension than n_embd\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (4,8,16)\n",
        "q = query(x) # (4,8,16)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # (4,8,16) @ (4,16,8) -> (4,8,8)\n",
        "\n",
        "# Mask future tokens\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "wei = wei.masked_fill(tril ==0, float('-inf'))\n",
        "wei = torch.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x) # (4,8,16)\n",
        "out = wei @ v # (4,8,8) @ (4,8,16) -> (4,8,16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "*   Attention is a communication mechanism for nodes in a directed graph. In the above case, each token has an edge that points to every token after it. For token `t`, this means there's an edge that points to it from every preceding token.\n",
        "*   As such, each node knows about the nodes that precede it, but no additional positional information. Specifically, `t` doesn't know that another node, let's say `t2`, is 3 nodes behind. `t` just knows that `t2` comes before itself.\n",
        "*   Self attention means that the keys are produced from the same source as the queries. Cross-attention means that the keys and queries are produced from different sources. The keys describe what you're \"looking\" for and the queries describe what it \"is\". An example of cross-attention is translation where the keys come from the source language, and the queries come from the language to be translated into.\n",
        "*   \"Scaled\" self attention is ensuring that the weight matrix has a variance of 1 before softmax() is applied. We do this by dividing the product of Key and Query by squareroot of the head size. The reasoning is that softmax will converge towards 1-hot vectors, vectors that are all 0s except for one 1, if the variance is high. This is bad at initialization because the model doesn't get a chance to \"learn\" about all the tokens equally, and decide how to adjust the weights based on backpropagation, instead it will overindex on specific tokens simply because of how it was randomly initialized.\n"
      ],
      "metadata": {
        "id": "iZCLlg0AvAmU"
      }
    }
  ]
}